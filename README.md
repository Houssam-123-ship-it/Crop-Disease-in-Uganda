Perfect addition ğŸŒ
Hereâ€™s your **updated final version** of the README for the **Crop Disease in Uganda â€” WQU Work Simulation Project**, now including the **Data Pollution & Power (2022)** white paper context in a professional and integrated way under a dedicated sustainability section.

---

# ğŸŒ¿ Crop Disease Classification in Uganda â€” WQU Work Simulation

## ğŸ“˜ Project Overview

This project is part of the **WorldQuant University (WQU) Data Science Work Simulation** and is inspired by real-world challenges in **agriculture and computer vision**.

The goal is to build a **deep learning model** that classifies images of **cassava plants in Uganda** into **five categories** â€” identifying whether a crop is healthy or affected by a specific disease.

Throughout the project, I worked with **Convolutional Neural Networks (CNNs)** and **Transfer Learning** techniques to improve accuracy and efficiency while preventing model overfitting.

---

## ğŸ¯ Learning Objectives

### ğŸ§© Part 1 â€” Data Exploration & Preparation

* Explore the dataset of crop disease images.
* Check image properties, data balance, and class distribution.
* Normalize pixel values and **balance unbalanced classes** through undersampling.
* Understand the impact of **data imbalance** on model performance.

**New Terms:**
`Unbalanced classes`, `Undersampling`

---

### ğŸ§  Part 2 â€” Building a CNN from Scratch

* Convert images from grayscale to RGB and **resize them for uniformity**.
* Create a transformation pipeline to prepare data for training.
* Build a **Convolutional Neural Network** to classify crop diseases into five classes.
* Train and evaluate the model using **learning curves** to detect overfitting.

**New Terms:**
`Overfitting`, `Learning Curve`

---

### âš™ï¸ Part 3 â€” Transfer Learning & Callbacks

* Load and preprocess cassava plant images for training.
* Use **Transfer Learning** by adapting a **pre-trained image classification model** (e.g., ResNet or MobileNet).
* Apply **Callbacks** to enhance model optimization:

  * **Learning Rate Scheduling** â€” dynamically adjust learning rate during training.
  * **Model Checkpointing** â€” save the best-performing model automatically.
  * **Early Stopping** â€” stop training when validation accuracy stops improving.
* Evaluate model performance using **k-fold cross-validation** for robustness.
* Generate predictions and prepare a formatted **competition submission file**.

**New Terms:**
`Transfer Learning`, `Callbacks`, `Learning Rate Scheduling`, `Checkpointing`, `Early Stopping`

---

## ğŸ§  Key Competencies Gained

**1. Deep Learning & CNN Design**

* Built convolutional architectures for multiclass image classification.
* Understood how convolution, pooling, and activation layers extract spatial features.

**2. Transfer Learning Mastery**

* Utilized pre-trained models (ResNet, VGG, or MobileNet) to improve training efficiency.
* Adapted model layers for new datasets and tasks.

**3. Model Optimization & Regularization**

* Applied **callbacks** for smarter training management.
* Implemented **early stopping** and **checkpointing** to prevent overfitting.
* Managed **learning rate schedules** to stabilize convergence.

**4. Data Preprocessing & Balancing**

* Detected unbalanced class distributions.
* Applied normalization and undersampling for fair model learning.

**5. Evaluation & Cross-validation**

* Used **k-fold validation** for robust performance estimation.
* Interpreted metrics like accuracy, loss, and learning curves for tuning decisions.

---

## ğŸ§° Tools & Technologies

| Category          | Tools                         |
| ----------------- | ----------------------------- |
| Language          | Python                        |
| Deep Learning     | TensorFlow / Keras or PyTorch |
| Image Processing  | Pillow (PIL), OpenCV          |
| Data Manipulation | NumPy, Pandas                 |
| Visualization     | Matplotlib, Seaborn           |
| Environment       | Jupyter Notebook              |

---

## ğŸ—ï¸ Repository Structure

```
ğŸ“‚ crop-disease-uganda/
â”œâ”€â”€ ğŸ“ Notebooks/
â”‚   â”œâ”€â”€ NB1-1-explore-dataset .ipynb
â”‚   â”œâ”€â”€ NB2-2-multiclass-classification.ipynb
â”‚   â”œâ”€â”€ NB3/3-transfer-learning (1).ipynb , training.py
â”‚   â”œâ”€â”€ NB3-4-callbacks.ipynb
â””â”€â”€ README.md
```

---

## ğŸš€ How to Run

1. **Clone the repository:**

   ```bash
   git clone https://github.com/Houssam-123-ship-it/crop-disease-uganda.git
   cd crop-disease-uganda
   ```

2. **Install dependencies:**

   ```bash
   pip install tensorflow pillow numpy pandas matplotlib seaborn
   ```

3. **Download dataset:**

   * Use the cassava disease dataset (publicly available via academic data sources or Kaggle).
   * Place it under a `data/` folder.

4. **Run notebooks in order:**

   ```
   NB1 â†’ NB2 â†’ NB3 â†’ NB4
   ```

## ğŸ“Š Results Summary

| Model                      | Technique          | Accuracy | Notes                                  |
| -------------------------- | ------------------ | -------- | -------------------------------------- |
| CNN (scratch)              | Basic architecture | ~80%     | Prone to overfitting                   |
| Transfer Learning (ResNet) | With Callbacks     | ~92%     | Faster convergence, stable performance |

---

## ğŸ’¡ Quick Tip â€” Efficient Training

Training CNNs on large image datasets is computationally demanding.
To save time, I leveraged a **pre-trained model** and implemented **callbacks** such as **EarlyStopping**, **ModelCheckpoint**, and **LearningRateScheduler**.

This approach allowed me to **reduce training time**, avoid **overfitting**, and maintain **high accuracy** â€” an essential practice in real-world ML workflows.

---

## ğŸŒ Ethical & Sustainability Perspective â€” *Data Pollution & Power (2022)*

The following excerpt comes from [**Data Pollution & Power (2022)**](https://gryhasselbalch.com/books/data-pollution-power-a-white-paper-for-a-global-sustainable-development-agenda-on-ai/), a white paper published by the **Bonn Sustainable AI Lab, Institute for Science and Ethics, University of Bonn**.

> â€œData pollutionâ€ refers to the **adverse environmental and societal impact** caused by generating, storing, handling, and processing digital data. It highlights the environmental costs of AI and Big Data, the unequal distribution of these costs across the Global North and South, and the **power dynamics** between governments, corporations, and individuals in managing data resources.
>
> The report emphasizes the need for a **holistic global approach** to AI sustainability â€” balancing technological advancement with ecological responsibility and social equity.

This project aligns with those principles by promoting **efficient AI practices**, such as:

* Using **Transfer Learning** instead of training from scratch to reduce computational waste.
* Employing **Early Stopping** to minimize unnecessary energy use during training.
* Raising awareness of the **ethical implications** of large-scale AI model deployment.

**Reference:**
Hasselbalch, G. (2022). *Data Pollution & Power â€“ White Paper for a Global Sustainable Agenda on AI.* The Sustainable AI Lab, Bonn University.

---

## ğŸ Conclusion

This project strengthened my understanding of **computer vision**, **transfer learning**, and **sustainable AI practices**.
It reflects the ability to handle end-to-end ML workflows â€” from preprocessing and model development to optimization and evaluation â€” while maintaining an awareness of AIâ€™s **environmental and ethical impact**.

---

## ğŸ‘¤ Author

**Houssam Kichchou**
ğŸŒ [GitHub: Houssam-123-ship-it](https://github.com/Houssam-123-ship-it)
ğŸ’¼ [LinkedIn](https://www.linkedin.com/in/houssam-kichchou)
ğŸ“ WorldQuant University â€” Data Science Work Simulation 2025

---

